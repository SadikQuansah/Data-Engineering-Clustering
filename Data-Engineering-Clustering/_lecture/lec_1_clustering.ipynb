{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "The best resource for clustering is [The SKLearn page](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "In the pre-lecture, you read about K-Means and how it can cluster well on globular clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means example\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.decomposition as dec\n",
    "import sklearn.cluster as clu\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.model_selection as ms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "N_CLUSTERS=3\n",
    "\n",
    "X, y = ds.make_blobs(\n",
    "    n_samples=200, n_features=2,\n",
    "    centers=N_CLUSTERS, cluster_std=1.5,\n",
    ")\n",
    "\n",
    "def relabel(cl):\n",
    "    \"\"\"Relabel a clustering with three clusters\n",
    "    to match the original classes.\"\"\"\n",
    "    if np.max(cl) != 2:\n",
    "        return cl\n",
    "    perms = np.array(list(permutations((0, 1, 2))))\n",
    "    i = np.argmin([np.sum(np.abs(perm[cl] - y))\n",
    "                   for perm in perms])\n",
    "    p = perms[i]\n",
    "    return p[cl]\n",
    "\n",
    "def display_clustering(labels, title):\n",
    "    \"\"\"Plot the data points with the cluster colors.\"\"\"\n",
    "    # We relabel the classes when there are 3 clusters\n",
    "    labels = relabel(labels)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "    # Display the points with the true labels on the\n",
    "    # left, and with the clustering labels on the\n",
    "    # right.\n",
    "    for ax, c, title in zip(axes,\n",
    "            [y, labels],\n",
    "            [\"True labels\", title]):\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=c, s=30,\n",
    "                   linewidths=0, cmap=plt.cm.rainbow)\n",
    "        ax.set_title(title)\n",
    "\n",
    "km = clu.KMeans(n_clusters=N_CLUSTERS)\n",
    "km.fit(X)\n",
    "display_clustering(km.labels_, \"KMeans(5)\")\n",
    "\n",
    "# However, K-Means has a few flaws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An Overview of Clustering Algorithms\n",
    "\n",
    "Let's see how other clustering algorithms do on different Datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "podoc": {
     "output_text": "<matplotlib.figure.Figure at 0x80f4da0>"
    }
   },
   "outputs": [],
   "source": [
    "# example of a clustering algorithm\n",
    "# warning: This will take some time :)\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                'min_samples': 20,\n",
    "                'xi': 0.05,\n",
    "                'min_cluster_size': 0.1}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2,\n",
    "                     'min_samples': 20, 'xi': 0.25}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2,\n",
    "              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2,\n",
    "             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    optics = cluster.OPTICS(min_samples=params['min_samples'],\n",
    "                            xi=params['xi'],\n",
    "                            min_cluster_size=params['min_cluster_size'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('OPTICS', optics),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K means \n",
    "- default algorithms that is used used first - scales well\n",
    "- assumes distance is euclidean\n",
    "- assumes circular clusters - thus nested circles, halfmoon and varying size of clusters doesn't perform well\n",
    "- assumes that clusters are similarly sized/shaped which causes issues with clusters with high size/shape variance\n",
    "- handles random noise pretty well \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main takeaways can be seen in this table from [this page](https://scikit-learn.org/stable/modules/clustering.html):\n",
    "\n",
    "<img src=\"../assets/clustering_table.png\" style=\"max-width:800px\" />\n",
    "\n",
    "## Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering starts with every point being its own cluster, then merges best cluster candidates together until only $k$ are left.\n",
    "\n",
    "The main advantage of this method is that you can **use many metrics on it** easily. It is also robust to variant cluster sizes and shapes -- unlike K-means which assumes roughly equally shaped circular clusters.\n",
    "\n",
    "The last advantage of this method is that, like K-means it's **fast**. Many clustering algorithms are very slow, so scalable algorithms are a valuable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agglomerative Clustering\n",
    "\n",
    "- Start with every point in dataset being it's own cluster\n",
    "- if you have K clusters, you take all points (who are their own clusters) and take the 2 closest points and merge their clusters into 1 and repeat until there are k left clusters \n",
    "\n",
    "- Works better for k means in certain instances (nested circles and half moons)\n",
    "- Clusters of different shape and sizes can be capted more efficiently than with K means\n",
    "- Cares about bridges between points. Might be skewed if one point is between two clusters\n",
    "- Does not perform well on pure noise (image is all blue)\n",
    "\n",
    "Two main parameters + K (number of clusters) in Hierarchical Agglomerative Clustering\n",
    "- Linkage - how you link to the next point\n",
    "- Affinity - vector distance calculation - how we calculate which is the closest point\n",
    "\n",
    "\n",
    "\n",
    "One time to use KMeans over Agglomerative clustering: \n",
    "- when you want to use cluster centers- easier to label new points in data set with kmeans than heirarchichal clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical clustering\n",
    "import time as time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "### In this example single linkage merges the whole roll\n",
    "### Others don't and merge across jumps creating fake clusters\n",
    "\n",
    "N_CLUSTERS = 3\n",
    "\n",
    "# single, complete, average, ward\n",
    "LINKAGE = 'single' \n",
    "#LINKAGE = 'average' \n",
    "#LINKAGE = 'ward' \n",
    "\n",
    "\n",
    "AFFINITY = 'euclidean' # “euclidean”, “l1”, “l2”, “manhattan”, “cosine”\n",
    "\n",
    "# Generate data (swiss roll dataset)\n",
    "n_samples = 1500\n",
    "noise = 0.5\n",
    "X, _ = make_swiss_roll(n_samples, noise=noise)\n",
    "# # Make it thinner\n",
    "# X[:, 1] *= .5\n",
    "\n",
    "# Compute clustering\n",
    "ward = AgglomerativeClustering(\n",
    "    n_clusters=N_CLUSTERS, linkage=LINKAGE,\n",
    "    affinity=AFFINITY\n",
    "    \n",
    "    ).fit(X)\n",
    "label = ward.labels_\n",
    "\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "               color=plt.cm.jet(float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan, HDBScan and OPTICS\n",
    "\n",
    "SKlearn has an implementation for DBScan. There is also a modernized version in the [HDBScan library](https://github.com/scikit-learn-contrib/hdbscan).\n",
    "\n",
    "DBSCAN views clusters as areas of high density separated by areas of low density. This means it can learn $k$ by itself.\n",
    "\n",
    "This also means that some points can be left \"unclustered\".\n",
    "\n",
    "The main parameters are the `eps` (epsilon, minimum distance) and minimum number of samples to form a dense area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBScan, HDBScan & OPTICS - all clustering algorithms from the same family \n",
    "\n",
    "- Interesting algorithms because they do not require a K, they find it themselves\n",
    "- sees clusters as areas of high density separated by areas of low density - how they find K\n",
    "\n",
    "- Take in 2 main params: \n",
    "- EPS (epsilon) - min distance to consider a dense area\n",
    "- min number of samples to form a dense area\n",
    "\n",
    "HDBScan almost a merge of DBScan and agglomerative clustering\n",
    "\n",
    "DBScan - closer to k mean\n",
    "\n",
    "**These algos can leave points out of clusters - helps with outliers - black points in the table images\n",
    "\n",
    "- They are unclustered because of their density ratio. - Depends on epsilon - if low (more unclustered)\n",
    "\n",
    "main real reason to use - other than no # of clusters determined\n",
    "- outlier detection - a point that is far away from common points (clusters) of data \n",
    "    - should be looked at\n",
    "    \n",
    "- Clustering - very important to use visualization to see clusters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan, hdbscan and optics\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "EPS = 0.3\n",
    "MIN_SAMPLES = 10\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
