{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "We saw before that the notion of \"distance\" between vector-valued objects is not simple like in scalar real numbers. There is more information on this [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html).\n",
    "\n",
    "Most people associate distance between two points to the `euclidean` distance (the hypotenuse of their coordinates). Others include:\n",
    "\n",
    "- Cosine distance (angle between vectors)\n",
    "- Manhattan AKA \"cityblock\" or \"taxicab\" (euclidean by going along square grid)\n",
    "- Minkowsky (uses a parameter p to generalize between manhattan and euclidean)\n",
    "- Mahalanobis (measures how many standard deviations away from each other the points are)\n",
    "\n",
    "The cosine distance is not a _proper_ distance metric. So is not always included in metric packages. The good news is that the euclidean distance on a normalized vector is equivalent.\n",
    "\n",
    "## Relative Scale\n",
    "\n",
    "This also means that if relative scales on features are different, they will \"pull too hard\" and destroy any chance you have of success.\n",
    "\n",
    "One way to do this is to normalize each feature.\n",
    "\n",
    "One good way to merge all of the different data types together into a clustering is by using [Gower Distance](https://dpmartin42.github.io/posts/r/cluster-mixed-types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dpmartin42.github.io/ - clustering info (in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative scale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "sns.set()\n",
    "\n",
    "N_CLUSTERS = 5\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=N_CLUSTERS,\n",
    "                      cluster_std=0.70, random_state=33)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],s=50)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    "y_means = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1],c=y_means,s=50,cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:,0],centers[:,1],c='black',s=200,alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rescale X \n",
    "X[:,1] *= 50\n",
    "\n",
    "### Do the same as before, now it fails\n",
    "kmeans2 = KMeans(n_clusters=N_CLUSTERS)\n",
    "kmeans2.fit(X)\n",
    "y_means = kmeans2.predict(X)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=y_means,s=50,cmap='viridis')\n",
    "\n",
    "centers = kmeans2.cluster_centers_\n",
    "plt.scatter(centers[:,0],centers[:,1],c='black',s=200,alpha=0.5);\n",
    "\n",
    "\n",
    "# The above fails to cluster because distances on the y-axis are now so much larger\n",
    "#  that it's easier to jump across clusters than to traverse y-axis.\n",
    "\n",
    "# ie easier to move along the axis that has a small scale - need to scale both axes!!\n",
    "\n",
    "# REMINDER:\n",
    "# Always normalize distances before using them in clustering!\n",
    "# Unlike regression, where coefficients \"scale\" the features,\n",
    "# when taking distances you need to fix it yourself beforehand.\n",
    "\n",
    "\n",
    "# This is good reminder to visualize the clusters - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Clustering\n",
    "\n",
    "Information can be found [here](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating clustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "\n",
    "def uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n",
    "                             fixed_n_classes=None, n_runs=5, seed=42):\n",
    "    \"\"\"Compute score for 2 random uniform cluster labelings.\n",
    "\n",
    "    Both random labelings have the same number of clusters for each value\n",
    "    possible value in ``n_clusters_range``.\n",
    "\n",
    "    When fixed_n_classes is not None the first labeling is considered a ground\n",
    "    truth class assignment with fixed number of classes.\n",
    "    \"\"\"\n",
    "    random_labels = np.random.RandomState(seed).randint\n",
    "    scores = np.zeros((len(n_clusters_range), n_runs))\n",
    "\n",
    "    if fixed_n_classes is not None:\n",
    "        labels_a = random_labels(low=0, high=fixed_n_classes, size=n_samples)\n",
    "\n",
    "    for i, k in enumerate(n_clusters_range):\n",
    "        for j in range(n_runs):\n",
    "            if fixed_n_classes is None:\n",
    "                labels_a = random_labels(low=0, high=k, size=n_samples)\n",
    "            labels_b = random_labels(low=0, high=k, size=n_samples)\n",
    "            scores[i, j] = score_func(labels_a, labels_b)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def ami_score(U, V):\n",
    "    return metrics.adjusted_mutual_info_score(U, V)\n",
    "\n",
    "score_funcs = [\n",
    "    metrics.adjusted_rand_score,\n",
    "    metrics.v_measure_score,\n",
    "    ami_score,\n",
    "    metrics.mutual_info_score,\n",
    "]\n",
    "\n",
    "# 2 independent random clusterings with equal cluster number\n",
    "\n",
    "n_samples = 100\n",
    "n_clusters_range = np.linspace(2, n_samples, 10).astype(int)\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plots = []\n",
    "names = []\n",
    "for score_func in score_funcs:\n",
    "    print(\"Computing %s for %d values of n_clusters and n_samples=%d\"\n",
    "          % (score_func.__name__, len(n_clusters_range), n_samples))\n",
    "\n",
    "    t0 = time()\n",
    "    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    plots.append(plt.errorbar(\n",
    "        n_clusters_range, np.median(scores, axis=1), scores.std(axis=1))[0])\n",
    "    names.append(score_func.__name__)\n",
    "\n",
    "plt.title(\"Clustering measures for 2 random uniform labelings\\n\"\n",
    "          \"with equal number of clusters\")\n",
    "plt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)\n",
    "plt.ylabel('Score value')\n",
    "plt.legend(plots, names)\n",
    "plt.ylim(bottom=-0.05, top=1.05)\n",
    "\n",
    "\n",
    "# Random labeling with varying n_clusters against ground class labels\n",
    "# with fixed number of clusters\n",
    "\n",
    "n_samples = 1000\n",
    "n_clusters_range = np.linspace(2, 100, 10).astype(int)\n",
    "n_classes = 10\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plots = []\n",
    "names = []\n",
    "for score_func in score_funcs:\n",
    "    print(\"Computing %s for %d values of n_clusters and n_samples=%d\"\n",
    "          % (score_func.__name__, len(n_clusters_range), n_samples))\n",
    "\n",
    "    t0 = time()\n",
    "    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n",
    "                                      fixed_n_classes=n_classes)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    plots.append(plt.errorbar(\n",
    "        n_clusters_range, scores.mean(axis=1), scores.std(axis=1))[0])\n",
    "    names.append(score_func.__name__)\n",
    "\n",
    "plt.title(\"Clustering measures for random uniform labeling\\n\"\n",
    "          \"against reference assignment with %d classes\" % n_classes)\n",
    "plt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)\n",
    "plt.ylabel('Score value')\n",
    "plt.ylim(bottom=-0.05, top=1.05)\n",
    "plt.legend(plots, names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All types of clustering evaluations for when you have ground truths. \n",
    "- depending on the number of clusters, they have a different behaviour - some stable some weak scaling relationship etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the number of clusters with silhouette analysis on KMeans clustering\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of `[-1, 1]`.\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "In this example the silhouette analysis is used to choose an optimal value for n_clusters. The silhouette plot shows that the n_clusters value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more ambivalent in deciding between 2 and 4.\n",
    "\n",
    "Also from the thickness of the silhouette plot the cluster size can be visualized. The silhouette plot for cluster 0 when n_clusters is equal to 2, is bigger in size owing to the grouping of the 3 sub clusters into one big cluster. However when the n_clusters is equal to 4, all the plots are more or less of similar thickness and hence are of similar sizes as can be also verified from the labelled scatter plot on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not most well versed on this: \n",
    "\n",
    "measure has rande between -1 and 1\n",
    "\n",
    "Silhouette coefficients near +1 indicate that they are far away from another cluster - tells us that we are more likely in an individual cluster\n",
    "\n",
    "Coeff near 0 shows they are probably assigned to the wrong cluster - we are on the decision boundary of two clusters\n",
    "\n",
    "** Students: can copy paste this code to perform their own silhouette analysis **\n",
    "\n",
    "Silhouette score is good - but we still ought to look at the vis to see which is better ie score is better for cluster of 2 but arguably the best cluster would be 4\n",
    "\n",
    "\n",
    "This is where you can use the elbow method - inflection point at the curve \n",
    "\n",
    "PLS VISUALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)  # For reproducibility\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the cluster with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
